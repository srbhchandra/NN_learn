{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\UCSD\\WInter 2016\\NN\\Homework\\hw2\n"
     ]
    }
   ],
   "source": [
    "cd D:\\UCSD\\WInter 2016\\NN\\Homework\\hw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pylab import *\n",
    "import math\n",
    "from scipy.special import expit\n",
    "import time\n",
    "import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import logsumexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Loading Data ######\n",
    "images1,  labels1  = mnist.loadMnistofDigit('training', digits=[1,2,3,4,5,6,7,8,9,0])\n",
    "images2,  labels2  = mnist.loadMnistofDigit('testing',  digits=[1,2,3,4,5,6,7,8,9,0])\n",
    "L1_size = 784   # +1 for bias\n",
    "#L2_size = 20    # +1 for bias\n",
    "L3_size = 10\n",
    "bias_size = 1    # At both Input and Hidden Layer\n",
    "total_size      = 60000\n",
    "training_size   = 50000\n",
    "validation_size = total_size - training_size\n",
    "testing_size    = 10000\n",
    "\n",
    "#Reshaping and dividing train to validation\n",
    "train_images = images1[:training_size].reshape(training_size, L1_size)\n",
    "train_labels = labels1[:training_size].reshape(training_size, )\n",
    "\n",
    "valid_images = images1[training_size:].reshape(validation_size, L1_size)\n",
    "valid_labels = labels1[training_size:].reshape(validation_size, )\n",
    "\n",
    "test_images  = images2[:testing_size].reshape(testing_size, L1_size)\n",
    "test_labels  = labels2[:testing_size].reshape(testing_size, )\n",
    "\n",
    "#Accounting for BIAS\n",
    "train_images = np.insert(train_images, 0, 1, axis=1)\n",
    "valid_images = np.insert(valid_images, 0, 1, axis=1)\n",
    "test_images  = np.insert(test_images , 0, 1, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#zscoring\n",
    "train_images_mean = np.mean(train_images, axis=1).reshape(training_size, 1)\n",
    "train_images_std  = np.std( train_images, axis=1).reshape(training_size, 1)\n",
    "train_images      = (train_images - train_images_mean)/train_images_std\n",
    "\n",
    "valid_images_mean = np.mean(valid_images, axis=1).reshape(validation_size, 1)\n",
    "valid_images_std  = np.std( valid_images, axis=1).reshape(validation_size, 1)\n",
    "valid_images      = (valid_images - valid_images_mean)/valid_images_std\n",
    "\n",
    "test_images_mean  = np.mean(test_images, axis=1).reshape(testing_size, 1)\n",
    "test_images_std   = np.std( test_images, axis=1).reshape(testing_size, 1)\n",
    "test_images       = (test_images - test_images_mean)/test_images_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def activation_function(A_j, fun_type):\n",
    "    if fun_type == \"sigmoid\":\n",
    "        temp  = np.matrix(expit(A_j))\n",
    "    elif fun_type == \"tanh\":\n",
    "        temp  = np.matrix(np.tanh(A_j))\n",
    "    elif fun_type == \"ReLU\":\n",
    "        temp  = np.matrix(np.maximum(np.zeros(A_j.shape), A_j))\n",
    "    elif fun_type == \"softmax\":\n",
    "        temp1 = np.matrix(logsumexp(A_j, axis=1)).transpose()\n",
    "        temp2 = np.dot(temp1, np.ones((1, A_j.shape[1])))\n",
    "        temp  = np.exp(A_j - temp2)\n",
    "        #den  = np.sum(np.exp(A_j))\n",
    "        #temp = np.matrix(np.exp(A_j))/den\n",
    "    else:\n",
    "        return null\n",
    "    return temp\n",
    "\n",
    "def activation_function_derivative(A_j, fun_type):\n",
    "    if fun_type == \"sigmoid\":\n",
    "        temp     =  np.matrix(expit(A_j))\n",
    "        temp_bar =  np.ones(np.shape(temp)) - temp\n",
    "        return np.multiply(temp, temp_bar)\n",
    "    elif fun_type == \"tanh\":\n",
    "        return np.matrix(np.multiply(1./np.cosh(A_j), 1./np.cosh(A_j)))\n",
    "    elif fun_type == \"ReLU\":\n",
    "        temp = np.matrix(np.maximum(np.zeros(A_j.shape), A_j))\n",
    "        temp[np.nonzero(temp)] = 1\n",
    "        return temp\n",
    "    elif fun_type == \"softmax\":\n",
    "        temp  = np.matrix(logsumexp(A_j, axis=1)).transpose()\n",
    "        temp2 = np.dot(temp, np.ones((1,A_j.shape[1])))\n",
    "        temp3 = np.exp(A_j-temp2)\n",
    "        temp3_bar =  np.ones(np.shape(temp3)) - temp3\n",
    "        return np.multiply(temp3, temp3_bar)\n",
    "    else:\n",
    "        return null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_NN(images, labels, ite, W_12, W_23, activation_hidden_type, activation_output_type):\n",
    "\n",
    "    for i in range(ite):\n",
    "        #----------Forward propogation-----------\n",
    "        #Stochastic gradient descent: random datapoint selection\n",
    "        index = np.random.randint(training_size)\n",
    "        X_1   = np.matrix(images[index])\n",
    "        #one-hot encoding of training data input labels\n",
    "        T = np.zeros((1, L3_size))\n",
    "        T[0, labels[index]] = 1.0\n",
    " \n",
    "        #Hidden layer outputs using activation function\n",
    "        A_2     = np.dot(X_1, W_12)\n",
    "        X_2     = activation_function(A_2, activation_hidden_type)\n",
    "        X_2     = np.insert(X_2, 0, 1, axis=1)  # Add BIAS term to Hidden Layer\n",
    "\n",
    "        #Output layer outputs using activation function: softmax\n",
    "        A_3     = np.dot(X_2, W_23)\n",
    "        X_3     = activation_function(A_3, activation_output_type)\n",
    "        \n",
    "        #---------Back Propogation--------------\n",
    "        #Delta at output layer\n",
    "        D_3     = T - X_3\n",
    "        \n",
    "        #Delta at hidden layer\n",
    "        D_2     = activation_function_derivative(A_2, activation_hidden_type)\n",
    "        # Add BIAS term but is it needed? Since we are not using that value to update W12. Instead reduce dim(W23) ?\n",
    "        D_2     = np.insert(D_2, 0, 1, axis=1)\n",
    "        D_2     = np.multiply(D_2, np.dot(D_3, np.transpose(W_23)))\n",
    "\n",
    "        #Weight matrix update at output layer with or without regularization\n",
    "        if lamda == 0:\n",
    "            W_23 += alpha*np.dot(np.transpose(X_2), D_3)\n",
    "        else:\n",
    "            W_23 = (1 - alpha*lamda)*W_23 + alpha*np.dot(np.transpose(X_2), D_3)\n",
    "        \n",
    "        #Weight matrix update at hidden layer with or without regularization\n",
    "        if lamda == 0:\n",
    "            W_12 += alpha*np.dot(np.transpose(X_1), D_2[0,1:])\n",
    "        else:\n",
    "            W_12 = (1 - alpha*lamda)*W_12 + alpha*np.dot(np.transpose(X_1), D_2[0,1:])\n",
    "\n",
    "    return W_12, W_23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_NN(images, labels, data_size, W_12, W_23, activation_hidden_type, activation_output_type):\n",
    "\n",
    "    VT   = np.zeros((data_size, L3_size))\n",
    "    VT[np.arange(data_size), labels] = 1.0        \n",
    "\n",
    "    VA_2     = np.dot(images, W_12)\n",
    "    VX_2     = activation_function(VA_2, activation_hidden_type)\n",
    "    VX_2     = np.insert(VX_2, 0, 1, axis=1) # Add BIAS term\n",
    "    VA_3     = np.dot(VX_2, W_23)\n",
    "    VX_3     = activation_function(VA_3, activation_output_type)\n",
    "\n",
    "    # Error (cost) computation with or without regularization\n",
    "    E        = sum(np.multiply(VT, np.log(VX_3 + ((10**-40)*np.ones(VX_3.shape)))))\n",
    "    if lamda != 0:\n",
    "        E   += lamda*(np.sum(np.multiply(W_12, W_12)) + np.sum(np.multiply(W_23, W_23)))/2    \n",
    "\n",
    "    VY       = np.argmax(VX_3, axis=1)\n",
    "    Err      = np.count_nonzero(VY.reshape(np.shape(labels)) - labels)\n",
    "    return E, Err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Initialization Block ####\n",
    "\n",
    "#alphas = [0.001, 0.0025, 0.005, 0.0075, 0.01, 0.015]\n",
    "#L2_sizes = [50, 100, 150, 200, 250, 300, 350]\n",
    "#activation_list = [\"sigmoid\", \"tanh\", \"ReLU\", \"softmax\"]\n",
    "#alphas   = [0.006]   # Learning Rate\n",
    "\n",
    "alphas = [0.002, 0.004, 0.006, 0.008, 0.01, 0.012, 0.014, 0.016, 0.018, 0.020]\n",
    "activation_list = [\"sigmoid\", \"tanh\", \"softmax\"]\n",
    "L2_size  = 150   # Number of Hidden Nodes\n",
    "lamda    = 0    # For regularization\n",
    "gamma    = 0    # For momentum\n",
    "activation_function_Hidden = activation_list[0]\n",
    "activation_function_Output = \"softmax\"\n",
    "\n",
    "if activation_function_Hidden == \"ReLU\":\n",
    "    training_block_size = 10\n",
    "else:\n",
    "    training_block_size = int(training_size/2)\n",
    "\n",
    "InitWeight_12     = 0.01*np.random.rand(L1_size + bias_size, L2_size)\n",
    "InitWeight_23     = 0.01*np.random.rand(L2_size + bias_size, L3_size)\n",
    "Convergence_check = 5\n",
    "Convergence_count = 3\n",
    "testErrorAlways   = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Alpha =  0.002  Activation Function Hidden =  sigmoid  and # Hidden Nodes =  150\n",
      "Iteration =  0  Training   Error =  -115100.841501"
     ]
    }
   ],
   "source": [
    "TrE       = []\n",
    "VE        = []\n",
    "TE        = []\n",
    "iterCount = []\n",
    "TrEFinal  = []\n",
    "VEFinal   = []\n",
    "TEFinal   = []\n",
    "W12Final  = []\n",
    "W23Final  = []\n",
    "iterCountFinal = []\n",
    "\n",
    "for activation_function_Hidden in activation_list:\n",
    "    for alpha in alphas:\n",
    "        print(\"### Alpha = \", alpha, \" Activation Function Hidden = \", activation_function_Hidden, \" and # Hidden Nodes = \", L2_size)\n",
    "        iter_count = 0\n",
    "        conv_count = 0\n",
    "        VE = []\n",
    "        TE = []\n",
    "        iterCount = []\n",
    "\n",
    "        weight_12 = np.copy(InitWeight_12)\n",
    "        weight_23 = np.copy(InitWeight_23)\n",
    "\n",
    "        ETr, ErrTr = test_NN(train_images, train_labels, training_size,   weight_12, weight_23, activation_function_Hidden, activation_function_Output)\n",
    "        EVl, ErrVl = test_NN(valid_images, valid_labels, validation_size, weight_12, weight_23, activation_function_Hidden, activation_function_Output)\n",
    "        print(\"Iteration = \", iter_count, \" Training   Error = \", ETr, \" :: Error Rate = \", ErrTr*100/training_size)\n",
    "        print(\"Iteration = \", iter_count, \" Validation Error = \", EVl, \" :: Error Rate = \", ErrVl*100/validation_size)\n",
    "\n",
    "        if testErrorAlways == 1:\n",
    "            ETest, ErrTest = test_NN(test_images, test_labels, testing_size, weight_12, weight_23, activation_function_Hidden, activation_function_Output)\n",
    "            print(\"Error for Test Images = \", ETest, \" :: Error Rate = \", ErrTest*100/testing_size)\n",
    "            TE.append(100 - ErrTest*100/testing_size)\n",
    "\n",
    "        TrE.append(100 - ErrTr*100/training_size)\n",
    "        VE.append(100 - ErrVl*100/validation_size)\n",
    "        iterCount.append(iter_count*training_block_size)\n",
    "\n",
    "        delta_E = abs(EVl)\n",
    "        while delta_E > Convergence_check or conv_count < Convergence_count:\n",
    "            weight_12, weight_23 = train_NN(train_images, train_labels, training_block_size, weight_12, weight_23, activation_function_Hidden, activation_function_Output)\n",
    "\n",
    "            iter_count += 1\n",
    "            E_initial  = EVl\n",
    "            ETr, ErrTr = test_NN(train_images, train_labels, training_size,   weight_12, weight_23, activation_function_Hidden, activation_function_Output)\n",
    "            EVl, ErrVl = test_NN(valid_images, valid_labels, validation_size, weight_12, weight_23, activation_function_Hidden, activation_function_Output)\n",
    "            print(\"Iteration = \", iter_count, \" Training   Error = \", ETr, \" :: Error Rate = \", ErrTr*100/training_size)\n",
    "            print(\"Iteration = \", iter_count, \" Validation Error = \", EVl, \" :: Error Rate = \", ErrVl*100/validation_size)\n",
    "            if testErrorAlways == 1:\n",
    "                ETest, ErrTest = test_NN(test_images, test_labels, testing_size, weight_12, weight_23, activation_function_Hidden, activation_function_Output)\n",
    "                print(\"Error for Test Images = \", ETest, \" :: Error Rate\", ErrTest*100/testing_size)\n",
    "                TE.append(100 - ErrTest*100/testing_size)\n",
    "\n",
    "            TrE.append(100 - ErrTr*100/training_size)\n",
    "            VE.append(100 - ErrVl*100/validation_size)\n",
    "            iterCount.append(iter_count*training_block_size)\n",
    "\n",
    "            delta_E = abs(E_initial - EVl)\n",
    "            if delta_E <= Convergence_check:\n",
    "                conv_count += 1\n",
    "            else:\n",
    "                conv_count = 0\n",
    "\n",
    "\n",
    "        print(\"Training   Error = {0} :: Error Rate = {1} :: Number of iterations = {2}\" .format(ETr, ErrTr*100/training_size,   iter_count*training_block_size))\n",
    "        print(\"Validation Error = {0} :: Error Rate = {1} :: Number of iterations = {2}\" .format(EVl, ErrVl*100/validation_size, iter_count*training_block_size))\n",
    "        if testErrorAlways == 0:\n",
    "            ETest, ErrTest = test_NN(test_images, test_labels, testing_size, weight_12, weight_23, activation_function_Hidden, activation_function_Output)\n",
    "            print(\"Test Error       = \", ETest, \" :: Error Rate = \", ErrTest*100/testing_size)\n",
    "            TE.append(100 - ErrTest*100/testing_size)\n",
    "        TrEFinal.append(TrE)\n",
    "        VEFinal.append(VE)\n",
    "        TEFinal.append(TE)\n",
    "        W12Final.append(weight_12)\n",
    "        W23Final.append(weight_23)        \n",
    "        iterCountFinal.append(iterCount)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# File Write\n",
    "np.set_printoptions(threshold=200000)\n",
    "logs = open(\"Solutions_1.txt\", 'w')\n",
    "logs.write(\"TrEFinal\\n\" + str(TrEFinal))\n",
    "logs.write(\"VEFinal\\n\" + str(VEFinal))\n",
    "logs.write(\"TEFinal\\n\" + str(TEFinal))\n",
    "logs.write(\"W12Final\\n\" + str(W12Final[0]))\n",
    "logs.write(\"W23Final\\n\" + str(W23Final[0]))\n",
    "logs.write(\"iterCountFinal\\n\" + str(iterCountFinal))\n",
    "logs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297 97.99 99.996\n",
      "148 98.06 99.902\n"
     ]
    }
   ],
   "source": [
    "n = argmax(VEFinal[0])\n",
    "print(len(VEFinal[0]), VEFinal[0][-1], TrEFinal[0][-1])\n",
    "print(n, VEFinal[0][n], TrEFinal[0][n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plotting Block\n",
    "plt.title('Alpha =  0.001, Activation Function Hidden = sigmoid, and # Hidden Nodes = 150')\n",
    "plt.plot(iterCountFinal[0][1:], TrEFinal[0][1:], color=\"blue\", label=\"Training\")\n",
    "plt.plot(iterCountFinal[0][1:], VEFinal[0][1:],  color=\"red\" , label=\"Validation\")\n",
    "plt.ylabel('Training (b) / Validation (r) Accuracy Rates')\n",
    "plt.xlabel('Iteration Count')\n",
    "plt.grid(True)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
